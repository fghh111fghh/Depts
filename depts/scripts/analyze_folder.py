#!/usr/bin/env python
import os
import sys
import csv
import pickle
import math
import chardet
from datetime import datetime
from pathlib import Path
from collections import defaultdict

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
MIN_MATCHES = 3
MAX_MATCHES = 7

# –ü—Ä–∏—á–∏–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∞
SKIP_REASONS = {
    'NO_SCORE': '–ù–µ—Ç —Å—á–µ—Ç–∞',
    'NO_ODDS': '–ù–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤',
    'INSUFFICIENT_HISTORY': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏–∏',
    'INVALID_DATE': '–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –¥–∞—Ç–∞',
    'ENCODING_ERROR': '–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏',
    'OTHER': '–î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞'
}

PROBABILITY_BINS = [
    (0, 5), (5, 10), (10, 15), (15, 20), (20, 25),
    (25, 30), (30, 35), (35, 40), (40, 45), (45, 50),
    (50, 55), (55, 60), (60, 65), (65, 70), (70, 75),
    (75, 80), (80, 85), (85, 90), (90, 95), (95, 100)
]

# –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏ —Å —à–∞–≥–æ–º 10% (–±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è)
ODDS_BINS = [
    (1.00, 1.10), (1.10, 1.21), (1.21, 1.33), (1.33, 1.46), (1.46, 1.61),
    (1.61, 1.77), (1.77, 1.95), (1.95, 2.14), (2.14, 2.35), (2.35, 2.59),
    (2.59, 2.85), (2.85, 3.13), (3.13, 3.44), (3.44, 3.78), (3.78, 4.16),
    (4.16, 4.58), (4.58, 5.04), (5.04, 5.54), (5.54, 6.09), (6.09, 6.70),
    (6.70, 7.37), (7.37, 8.11), (8.11, 8.92), (8.92, 9.81), (9.81, 10.79),
    (10.79, 11.87), (11.87, 13.06), (13.06, float('inf'))
]


def get_probability_bin(prob):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–ª–æ–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (5% –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)"""
    for low, high in PROBABILITY_BINS:
        if low <= prob < high:
            return f"{low}-{high}%"
    return "95-100%"


def get_odds_bin(odds):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–ª–æ–∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å —à–∞–≥–æ–º 10%"""
    if odds is None:
        return None
    for low, high in ODDS_BINS:
        if low <= odds < high:
            if high == float('inf'):
                return f">{low:.2f}"
            return f"{low:.2f}-{high:.2f}"
    return f">{ODDS_BINS[-1][0]:.2f}"


def detect_delimiter(file_path):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV —Ñ–∞–π–ª–µ"""
    with open(file_path, 'r', encoding='utf-8-sig') as f:
        first_line = f.readline().strip()
        commas = first_line.count(',')
        semicolons = first_line.count(';')
        return ';' if semicolons > commas else ','


def detect_encoding(file_path):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ñ–∞–π–ª–∞"""
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 10000 –±–∞–π—Ç
        result = chardet.detect(raw_data)
        return result['encoding']


def get_odds_from_row(row, odds_type):
    """
    –ò—â–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫)

    Args:
        row: —Å—Ç—Ä–æ–∫–∞ CSV
        odds_type: —Ç–∏–ø –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ ('H' - –ü1, 'D' - –Ω–∏—á—å—è, 'A' - –ü2, 'OVER' - –¢–ë2.5)

    Returns:
        float or None
    """
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏
    odds_mapping = {
        'H': [
            # –û—Å–Ω–æ–≤–Ω—ã–µ –±—É–∫–º–µ–∫–µ—Ä—ã
            'B365H', 'BWH', 'IWH', 'LBH', 'PSH', 'WHH', 'SJH', 'VCH',
            # –°—Ä–µ–¥–Ω–∏–µ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ
            'AvgH', 'MaxH', 'BbAvH', 'BbMxH',
            # –î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            'BFH', 'BFEH', 'PSCH', 'BWCH', 'BFCH', 'WHCH', '1XBH', 'MaxCH', 'AvgCH'
        ],
        'D': [
            'B365D', 'BWD', 'IWD', 'LBD', 'PSD', 'WHD', 'SJD', 'VCD',
            'AvgD', 'MaxD', 'BbAvD', 'BbMxD',
            'BFD', 'BFED', 'PSCD', 'BWCD', 'BFCD', 'WHCD', '1XBD', 'MaxCD', 'AvgCD'
        ],
        'A': [
            'B365A', 'BWA', 'IWA', 'LBA', 'PSA', 'WHA', 'SJA', 'VCA',
            'AvgA', 'MaxA', 'BbAvA', 'BbMxA',
            'BFA', 'BFEA', 'PSCA', 'BWCA', 'BFCA', 'WHCA', '1XBA', 'MaxCA', 'AvgCA'
        ],
        'OVER': [
            # –û—Å–Ω–æ–≤–Ω—ã–µ
            'B365>2.5', 'P>2.5', 'Max>2.5', 'Avg>2.5',
            # BetBrain
            'BbMx>2.5', 'BbAv>2.5',
            # –î—Ä—É–≥–∏–µ
            'BFE>2.5', 'BFEC>2.5', 'PC>2.5', 'MaxC>2.5', 'AvgC>2.5'
        ],
        'UNDER': [
            'B365<2.5', 'P<2.5', 'Max<2.5', 'Avg<2.5',
            'BbMx<2.5', 'BbAv<2.5',
            'BFE<2.5', 'BFEC<2.5', 'PC<2.5', 'MaxC<2.5', 'AvgC<2.5'
        ]
    }

    for col in odds_mapping.get(odds_type, []):
        if col in row:
            value = row.get(col, '').strip()
            if value and value != 'NA' and value != '':
                try:
                    return float(value.replace(',', '.'))
                except (ValueError, TypeError):
                    continue
    return None


def safe_int(value):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
    if not value or str(value).strip() == '' or str(value).lower() == 'nan':
        return None
    try:
        return int(float(str(value).replace(',', '.')))
    except (ValueError, TypeError):
        return None


def parse_date(date_str):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    if not date_str or not isinstance(date_str, str):
        return None

    date_str = date_str.strip()

    for fmt in ('%d/%m/%Y', '%d/%m/%y', '%Y-%m-%d'):
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue

    return None


def get_poisson_probs(l_home, l_away):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –ü—É–∞—Å—Å–æ–Ω—É"""
    try:
        l_home = max(float(l_home), 0.1)
        l_away = max(float(l_away), 0.1)

        exp_home = math.exp(-l_home)
        exp_away = math.exp(-l_away)

        max_goals = 5
        factorials = [math.factorial(i) for i in range(max_goals + 1)]
        home_powers = [l_home ** i for i in range(max_goals + 1)]
        away_powers = [l_away ** i for i in range(max_goals + 1)]

        over25_yes = 0.0

        for h in range(max_goals + 1):
            p_h = (exp_home * home_powers[h]) / factorials[h]
            for a in range(max_goals + 1):
                p_a = (exp_away * away_powers[a]) / factorials[a]
                probability = p_h * p_a * 100

                if (h + a) > 2.5:
                    over25_yes += probability

        return {'over25_yes': over25_yes}

    except Exception:
        return {'over25_yes': 50.0}


def calculate_poisson_lambda_from_history(home_history, away_history, league_avg_home, league_avg_away):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ª—è–º–±–¥—ã –ü—É–∞—Å—Å–æ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        if len(home_history) < MIN_MATCHES or len(away_history) < MIN_MATCHES:
            return None

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        l_avg_home_goals = max(league_avg_home, 1.0)
        l_avg_away_goals = max(league_avg_away, 0.8)
        l_avg_home_conceded = l_avg_away_goals
        l_avg_away_conceded = l_avg_home_goals

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ MAX_MATCHES –º–∞—Ç—á–µ–π
        home_recent = home_history[-MAX_MATCHES:] if len(home_history) > MAX_MATCHES else home_history
        away_recent = away_history[-MAX_MATCHES:] if len(away_history) > MAX_MATCHES else away_history

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö–æ–∑—è–µ–≤
        h_avg_scored = sum(m['home_score'] for m in home_recent) / len(home_recent)
        h_avg_conceded = sum(m['away_score'] for m in home_recent) / len(home_recent)
        h_avg_scored = max(h_avg_scored, 0.5)
        h_avg_conceded = max(h_avg_conceded, 0.5)
        home_attack = h_avg_scored / l_avg_home_goals
        home_defense = h_avg_conceded / l_avg_home_conceded

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–æ—Å—Ç–µ–π
        a_avg_scored = sum(m['away_score'] for m in away_recent) / len(away_recent)
        a_avg_conceded = sum(m['home_score'] for m in away_recent) / len(away_recent)
        a_avg_scored = max(a_avg_scored, 0.3)
        a_avg_conceded = max(a_avg_conceded, 0.5)
        away_attack = a_avg_scored / l_avg_away_goals
        away_defense = a_avg_conceded / l_avg_away_conceded

        lambda_home = home_attack * away_defense * l_avg_home_goals
        lambda_away = away_attack * home_defense * l_avg_away_goals

        lambda_home = max(min(lambda_home, 3.5), 0.5)
        lambda_away = max(min(lambda_away, 3.0), 0.3)

        return {
            'home_lambda': round(lambda_home, 2),
            'away_lambda': round(lambda_away, 2)
        }

    except Exception:
        return None


def analyze_csv_file(file_path):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω CSV —Ñ–∞–π–ª (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)
    """
    print(f"\n--- –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {os.path.basename(file_path)} ---")

    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º
    skip_stats = defaultdict(int)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists(file_path):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
    try:
        encoding = detect_encoding(file_path)
        print(f"üìÑ –ö–æ–¥–∏—Ä–æ–≤–∫–∞: {encoding}")
    except:
        encoding = 'utf-8-sig'
        print(f"üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {encoding}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    try:
        delimiter = detect_delimiter(file_path)
        print(f"üìä –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{delimiter}'")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è: {e}")
        skip_stats['ENCODING_ERROR'] += 1
        return {
            'file_name': os.path.basename(file_path),
            'total_matches': 0,
            'analyzed': 0,
            'skipped': 0,
            'errors': 1,
            'skip_stats': dict(skip_stats),
            'predictions': [],
            'stats': {}
        }

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
    all_matches = []

    try:
        with open(file_path, mode='r', encoding=encoding, errors='replace') as f:
            reader = csv.DictReader(f, delimiter=delimiter)

            fieldnames = reader.fieldnames
            if not fieldnames:
                print("‚ùå –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
                skip_stats['OTHER'] += 1
                return {
                    'file_name': os.path.basename(file_path),
                    'total_matches': 0,
                    'analyzed': 0,
                    'skipped': 0,
                    'errors': 1,
                    'skip_stats': dict(skip_stats),
                    'predictions': [],
                    'stats': {}
                }

            print(f"üìã –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {len(fieldnames)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (—Ç–æ–ª—å–∫–æ –¥–∞—Ç–∞ –∏ –∫–æ–º–∞–Ω–¥—ã)
            required_cols = ['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG']
            missing_cols = [col for col in required_cols if col not in fieldnames]

            if missing_cols:
                print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
                skip_stats['OTHER'] += 1
                return {
                    'file_name': os.path.basename(file_path),
                    'total_matches': 0,
                    'analyzed': 0,
                    'skipped': 0,
                    'errors': 1,
                    'skip_stats': dict(skip_stats),
                    'predictions': [],
                    'stats': {}
                }

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏
            for row in reader:
                date_str = row.get('Date', '').strip()
                dt = parse_date(date_str)
                if not dt:
                    skip_stats['INVALID_DATE'] += 1
                    continue

                # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                odds_h = get_odds_from_row(row, 'H')
                odds_d = get_odds_from_row(row, 'D')
                odds_a = get_odds_from_row(row, 'A')
                odds_over = get_odds_from_row(row, 'OVER')
                odds_under = get_odds_from_row(row, 'UNDER')

                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —Ç–æ—Ç–∞–ª–∞
                if odds_over is None and odds_under is not None:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º UNDER –≤ OVER (1/UNDER)
                    try:
                        odds_over = 1.0 / odds_under
                    except:
                        pass

                all_matches.append({
                    'date': dt,
                    'date_str': date_str,
                    'home_team': row.get('HomeTeam', '').strip(),
                    'away_team': row.get('AwayTeam', '').strip(),
                    'fthg': safe_int(row.get('FTHG')),
                    'ftag': safe_int(row.get('FTAG')),
                    'odds_h': odds_h,
                    'odds_d': odds_d,
                    'odds_a': odds_a,
                    'odds_over': odds_over,
                    'odds_under': odds_under
                })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
            all_matches.sort(key=lambda x: x['date'])

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        skip_stats['ENCODING_ERROR'] += 1
        return {
            'file_name': os.path.basename(file_path),
            'total_matches': 0,
            'analyzed': 0,
            'skipped': 0,
            'errors': 1,
            'skip_stats': dict(skip_stats),
            'predictions': [],
            'stats': {}
        }

    total_rows = len(all_matches)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {total_rows}")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    stats = defaultdict(lambda: {'hits': 0, 'total': 0})
    predictions = []

    analyzed = 0
    errors = 0

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –∫–æ–Ω—Ü–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–∞—Ç—á–∏ –ø–µ—Ä–≤—ã–º–∏)
    for idx in range(total_rows - 1, -1, -1):
        match = all_matches[idx]

        try:
            if match['fthg'] is None or match['ftag'] is None:
                skip_stats['NO_SCORE'] += 1
                continue

            total_goals = match['fthg'] + match['ftag']

            if not match['odds_h'] or not match['odds_over']:
                skip_stats['NO_ODDS'] += 1
                continue

            # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∫–æ–º–∞–Ω–¥ –∏–∑ –ü–†–û–®–õ–´–• –º–∞—Ç—á–µ–π
            home_history = []
            away_history = []

            for prev_idx in range(idx):
                prev_match = all_matches[prev_idx]
                if prev_match['fthg'] is not None and prev_match['ftag'] is not None:
                    if prev_match['home_team'] == match['home_team']:
                        home_history.append({
                            'home_score': prev_match['fthg'],
                            'away_score': prev_match['ftag']
                        })
                    if prev_match['away_team'] == match['away_team']:
                        away_history.append({
                            'home_score': prev_match['fthg'],
                            'away_score': prev_match['ftag']
                        })

            # –°—Ä–µ–¥–Ω–∏–µ –ø–æ –ª–∏–≥–µ
            all_prev_matches = all_matches[:idx]
            if all_prev_matches:
                all_home_goals = [m['fthg'] for m in all_prev_matches if m['fthg'] is not None]
                all_away_goals = [m['ftag'] for m in all_prev_matches if m['ftag'] is not None]
                league_avg_home = sum(all_home_goals) / len(all_home_goals) if all_home_goals else 1.2
                league_avg_away = sum(all_away_goals) / len(all_away_goals) if all_away_goals else 1.0
            else:
                league_avg_home = 1.2
                league_avg_away = 1.0

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ª—è–º–±–¥—ã
            lambda_result = calculate_poisson_lambda_from_history(
                home_history, away_history,
                league_avg_home, league_avg_away
            )

            if lambda_result is None:
                skip_stats['INSUFFICIENT_HISTORY'] += 1
                continue

            lambda_home = lambda_result['home_lambda']
            lambda_away = lambda_result['away_lambda']

            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            probs = get_poisson_probs(lambda_home, lambda_away)
            over25_prob = probs['over25_yes']

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–ª–æ–∫–∏
            odds_h_bin = get_odds_bin(match['odds_h'])
            odds_over_bin = get_odds_bin(match['odds_over'])
            prob_bin = get_probability_bin(over25_prob)

            # –ö–ª—é—á –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            key = (odds_h_bin, odds_over_bin, prob_bin)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats[key]['total'] += 1
            if total_goals > 2.5:
                stats[key]['hits'] += 1

            predictions.append({
                'date': match['date_str'],
                'home_team': match['home_team'],
                'away_team': match['away_team'],
                'fthg': match['fthg'],
                'ftag': match['ftag'],
                'total_goals': total_goals,
                'odds_h': match['odds_h'],
                'odds_over': match['odds_over'],
                'odds_h_bin': odds_h_bin,
                'odds_over_bin': odds_over_bin,
                'over25_prob': over25_prob,
                'prob_bin': prob_bin,
                'hit': total_goals > 2.5
            })

            analyzed += 1

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏: {e}")
            skip_stats['OTHER'] += 1
            errors += 1

    total_skipped = sum(skip_stats.values())

    print(f"   ‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {analyzed}")
    print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {total_skipped}")
    for reason, count in skip_stats.items():
        if count > 0:
            print(f"      - {SKIP_REASONS[reason]}: {count}")

    return {
        'file_name': os.path.basename(file_path),
        'total_matches': total_rows,
        'analyzed': analyzed,
        'skipped': total_skipped,
        'errors': errors,
        'skip_stats': dict(skip_stats),
        'predictions': predictions,
        'stats': dict(stats)
    }


def analyze_folder(folder_path):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ CSV —Ñ–∞–π–ª—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ
    """
    print("\n" + "=" * 80)
    print(f"–ê–ù–ê–õ–ò–ó –ü–ê–ü–ö–ò: {folder_path}")
    print("=" * 80)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏
    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        return None

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
    csv_files = list(Path(folder_path).glob('*.csv'))

    if not csv_files:
        print(f"‚ùå –í –ø–∞–ø–∫–µ –Ω–µ—Ç CSV —Ñ–∞–π–ª–æ–≤")
        return None

    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤: {len(csv_files)}")

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º –≤ –ø–∞–ø–∫–µ
    folder_stats = {
        'folder_name': os.path.basename(folder_path),
        'folder_path': folder_path,
        'total_files': len(csv_files),
        'processed_files': 0,
        'files_with_errors': 0,
        'total_matches': 0,
        'total_analyzed': 0,
        'total_skipped': 0,
        'total_errors': 0,
        'files': [],
        'combined_stats': defaultdict(lambda: {'hits': 0, 'total': 0}),
        'combined_skip_stats': defaultdict(int),
        'predictions': []
    }

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for csv_file in sorted(csv_files):
        print(f"\n{'=' * 60}")
        print(f"–û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–ê: {csv_file.name}")
        print('=' * 60)

        file_result = analyze_csv_file(str(csv_file))

        if file_result:
            folder_stats['processed_files'] += 1
            folder_stats['total_matches'] += file_result['total_matches']
            folder_stats['total_analyzed'] += file_result['analyzed']
            folder_stats['total_skipped'] += file_result['skipped']
            folder_stats['total_errors'] += file_result['errors']

            if file_result['errors'] > 0:
                folder_stats['files_with_errors'] += 1

            folder_stats['files'].append({
                'file_name': file_result['file_name'],
                'total_matches': file_result['total_matches'],
                'analyzed': file_result['analyzed'],
                'skipped': file_result['skipped'],
                'errors': file_result['errors'],
                'skip_stats': file_result.get('skip_stats', {})
            })

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º
            for reason, count in file_result.get('skip_stats', {}).items():
                folder_stats['combined_skip_stats'][reason] += count

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–ª–æ–∫–∞–º
            for key, data in file_result['stats'].items():
                folder_stats['combined_stats'][key]['total'] += data['total']
                folder_stats['combined_stats'][key]['hits'] += data['hits']

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            folder_stats['predictions'].extend(file_result['predictions'][-10:])

    # –û–≥—Ä–∞–Ω–∏—á–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    if len(folder_stats['predictions']) > 100:
        folder_stats['predictions'] = folder_stats['predictions'][-100:]

    return folder_stats


def print_folder_stats(stats):
    """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–∞–ø–∫–µ"""
    print("\n" + "=" * 80)
    print(f"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–ê–ü–ö–ï: {stats['folder_name']}")
    print("=" * 80)

    print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {stats['total_files']}")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {stats['processed_files']}")
    print(f"   –§–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: {stats['files_with_errors']}")
    print(f"   –í—Å–µ–≥–æ –º–∞—Ç—á–µ–π: {stats['total_matches']}")
    print(f"   ‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {stats['total_analyzed']}")
    print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['total_skipped']}")
    print(f"   ‚ùå –û—à–∏–±–æ–∫: {stats['total_errors']}")

    if stats['total_matches'] > 0:
        analyzed_percent = (stats['total_analyzed'] / stats['total_matches']) * 100
        print(f"\nüìà –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {analyzed_percent:.1f}%")

    print("\nüìä –ü–†–ò–ß–ò–ù–´ –ü–†–û–ü–£–°–ö–ê:")
    total_skipped = stats['total_skipped']
    for reason_code, reason_name in SKIP_REASONS.items():
        count = stats['combined_skip_stats'].get(reason_code, 0)
        if count > 0:
            percent = (count / total_skipped) * 100 if total_skipped > 0 else 0
            print(f"   {reason_name}: {count} ({percent:.1f}%)")

    print("\n" + "=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–ê–ú:")
    print("=" * 80)

    for f in stats['files']:
        analyzed_percent = (f['analyzed'] / f['total_matches']) * 100 if f['total_matches'] > 0 else 0
        error_mark = " ‚ùå" if f['errors'] > 0 else ""
        print(f"{f['file_name']}{error_mark}: {f['analyzed']}/{f['total_matches']} = {analyzed_percent:.1f}%")

        # –î–µ—Ç–∞–ª–∏ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º –¥–ª—è —Ñ–∞–π–ª–∞
        if f.get('skip_stats'):
            for reason, count in f['skip_stats'].items():
                if count > 0:
                    print(f"      - {SKIP_REASONS[reason]}: {count}")

    if stats['combined_stats']:
        print("\n" + "=" * 80)
        print("–û–ë–™–ï–î–ò–ù–ï–ù–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ë–õ–û–ö–ê–ú")
        print("=" * 80)

        def sort_key(key):
            odds_h_bin, odds_over_bin, prob_bin = key
            odds_h_val = float(odds_h_bin.split('-')[0]) if odds_h_bin and '-' in odds_h_bin else 0
            odds_over_val = float(odds_over_bin.split('-')[0]) if odds_over_bin and '-' in odds_over_bin else 0
            prob_val = int(prob_bin.split('-')[0]) if prob_bin != '95-100%' else 95
            return (odds_h_val, odds_over_val, prob_val)

        sorted_keys = sorted(stats['combined_stats'].keys(), key=sort_key)

        total_analyzed = 0
        for key in sorted_keys:
            data = stats['combined_stats'][key]
            if data['total'] > 0:
                odds_h_bin, odds_over_bin, prob_bin = key
                hit_rate = (data['hits'] / data['total']) * 100
                print(
                    f"–ü1:{odds_h_bin} | –¢–ë:{odds_over_bin} | {prob_bin}: {data['hits']}/{data['total']} = {hit_rate:.1f}%")
                total_analyzed += data['total']

        print(f"\nüìä –í—Å–µ–≥–æ —É—á—Ç–µ–Ω–æ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ: {total_analyzed} –º–∞—Ç—á–µ–π")


def find_first_csv_file():
    """–ù–∞—Ö–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–π –ø–æ–ø–∞–≤—à–∏–π—Å—è CSV —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ ../all_matches/"""
    base_dir = Path(__file__).parent.parent
    all_matches_dir = base_dir / 'all_matches'

    if not all_matches_dir.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {all_matches_dir}")
        return None

    for league_dir in all_matches_dir.iterdir():
        if league_dir.is_dir():
            return str(league_dir)

    print("‚ùå –í –ø–∞–ø–∫–µ all_matches –Ω–µ—Ç –ø–æ–¥–ø–∞–ø–æ–∫ —Å –ª–∏–≥–∞–º–∏")
    return None


def main():
    import argparse

    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ CSV —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ')
    parser.add_argument('folder', nargs='?', default=None, help='–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å CSV —Ñ–∞–π–ª–∞–º–∏')

    args = parser.parse_args()

    # –ï—Å–ª–∏ –ø–∞–ø–∫–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –∏—â–µ–º ../all_matches/–ø–µ—Ä–≤—É—é_–ø–æ–ø–∞–≤—à—É—é—Å—è_–ø–∞–ø–∫—É
    if not args.folder:
        args.folder = find_first_csv_file()
        if args.folder:
            print(f"üìÅ –í—ã–±—Ä–∞–Ω–∞ –ø–∞–ø–∫–∞: {args.folder}")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–∞–ø–∫—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–ø–∫—É
    stats = analyze_folder(args.folder)

    if stats and stats['processed_files'] > 0:
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print_folder_stats(stats)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        base_dir = Path(__file__).parent.parent
        output_dir = base_dir / 'analysis_results'
        output_dir.mkdir(exist_ok=True)

        folder_name = stats['folder_name'].replace(' ', '_').replace('/', '_')
        output_file = output_dir / f"{folder_name}_analysis.pkl"

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–ª—é—á–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_stats = {}
        for key, value in stats['combined_stats'].items():
            odds_h_bin, odds_over_bin, prob_bin = key
            str_key = (str(odds_h_bin), str(odds_over_bin), str(prob_bin))
            save_stats[str_key] = value

        stats_for_save = {
            'folder_name': stats['folder_name'],
            'folder_path': stats['folder_path'],
            'total_files': stats['total_files'],
            'processed_files': stats['processed_files'],
            'files_with_errors': stats['files_with_errors'],
            'total_matches': stats['total_matches'],
            'total_analyzed': stats['total_analyzed'],
            'total_skipped': stats['total_skipped'],
            'total_errors': stats['total_errors'],
            'skip_stats': dict(stats['combined_skip_stats']),
            'files': stats['files'],
            'combined_stats': save_stats,
            'predictions': stats['predictions']
        }

        with open(output_file, 'wb') as f:
            pickle.dump(stats_for_save, f)

        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
        print("\n" + "üéØ" * 10)
        print("–ê–ù–ê–õ–ò–ó –ü–ê–ü–ö–ò –ó–ê–í–ï–†–®–ï–ù")
        print("üéØ" * 10)
    else:
        print("\n‚ùå –ù–ï –£–î–ê–õ–û–°–¨ –û–ë–†–ê–ë–û–¢–ê–¢–¨ –ù–ò –û–î–ò–ù –§–ê–ô–õ")


if __name__ == "__main__":
    main()